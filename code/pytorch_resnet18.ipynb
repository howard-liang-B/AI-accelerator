{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\YOLOv8_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn, optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import models\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from datetime import datetime\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadro RTX 3000 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(torch.cuda.get_device_name())\n",
    "else:\n",
    "    device = \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "## Training set has 100 instances.\n",
      "## Validation set has 10 instances.\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, ), (0.5, ))])\n",
    "\n",
    "# Create dataset(use 100 data for my laptop)\n",
    "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "valid_set = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
    "train_set = Subset(train_set, list(range(100)))\n",
    "valid_set = Subset(valid_set, list(range(100, 110)))\n",
    "\n",
    "# Create data loaders for our datasets\n",
    "train_loader = DataLoader(train_set, batch_size=5, shuffle=False)\n",
    "valid_loader = DataLoader(valid_set, batch_size=5, shuffle=False)\n",
    "\n",
    "print(f'## Training set has {len(train_set)} instances.')\n",
    "print(f'## Validation set has {len(valid_set)} instances.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(weights=\"IMAGENET1K_V1\", progress=True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function(Criterion) & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    sum_loss, sum_acc = 0.0, 0.0\n",
    "    running_loss,running_acc = 0.0, 0.0\n",
    "    last_loss, last_acc = 0.0, 0.0\n",
    "\n",
    "    START_TIME = time()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        sum_loss += loss.item()\n",
    "        running_acc += accuracy_score(labels.cpu(), outputs.argmax(dim=1).cpu())\n",
    "        sum_acc += accuracy_score(labels.cpu(), outputs.argmax(dim=1).cpu())\n",
    "        if i % 10 == 9:\n",
    "            last_loss = running_loss/10\n",
    "            last_acc = running_acc/10\n",
    "            # print(f' - Batch {i+1} loss: {last_loss:.4f} / accuracy: {last_acc:.4f}')\n",
    "\n",
    "            tb_x = epoch_index * len(train_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            tb_writer.add_scalar('Accuracy/train', last_acc, tb_x)\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "    END_TIME = time()\n",
    "\n",
    "    count = len(train_loader.dataset)\n",
    "    return sum_loss/count, sum_acc/count, (END_TIME-START_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## start warm up\n",
      "## finished warm up\n",
      "EPOCH 1: Train Loss: 0.0014 / Valid Loss: 3.3157 / Train Accuracy: 0.2000 / Valid Accuracy: 0.4000 (0.3890 sec)\n",
      "EPOCH 2: Train Loss: 0.0011 / Valid Loss: 3.3276 / Train Accuracy: 0.2000 / Valid Accuracy: 0.4000 (0.3820 sec)\n",
      "EPOCH 3: Train Loss: 0.0009 / Valid Loss: 3.3384 / Train Accuracy: 0.2000 / Valid Accuracy: 0.4000 (0.3810 sec)\n",
      "EPOCH 4: Train Loss: 0.0008 / Valid Loss: 3.3499 / Train Accuracy: 0.2000 / Valid Accuracy: 0.4000 (0.3820 sec)\n",
      "EPOCH 5: Train Loss: 0.0007 / Valid Loss: 3.3612 / Train Accuracy: 0.2000 / Valid Accuracy: 0.4000 (0.3850 sec)\n",
      "== Total time: 1.9190 sec ==\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/resnet18_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "total_time = 0\n",
    "EPOCHS = 5\n",
    "best_vloss = 1_000_000.0\n",
    "\n",
    "# warm up \n",
    "print(f'## start warm up')\n",
    "dummy_data = torch.randn(1, 3, 32, 32).to(device)\n",
    "for _ in range(1000):\n",
    "    _ = model(dummy_data)\n",
    "print(f'## finished warm up')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'EPOCH {epoch_number+1}: ', end=\"\")\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss, avg_acc, train_time = train_one_epoch(epoch_number, writer)\n",
    "    total_time += train_time\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    running_vloss = 0.0\n",
    "    running_vacc = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(valid_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            vinputs, vlabels = vinputs.to(device), vlabels.to(device)\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels) # current batch valid loss\n",
    "            vacc = accuracy_score(vlabels.cpu(), voutputs.argmax(dim=1).cpu()) # current batch valid accuracy\n",
    "            running_vloss += vloss.item()\n",
    "            running_vacc += vacc\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    avg_vacc = running_vacc / (i + 1)\n",
    "    print(f'Train Loss: {avg_loss:.4f} / Valid Loss: {avg_vloss:.4f} / '\n",
    "      f'Train Accuracy: {avg_acc:.4f} / Valid Accuracy: {avg_vacc:.4f} ({train_time:.4f} sec)')\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_model = model\n",
    "        best_vloss = avg_vloss\n",
    "\n",
    "    epoch_number += 1\n",
    "print(f'== Total time: {total_time:.4f} sec ==')\n",
    "model_path = f'models/model_renet18_{timestamp}_{epoch_number}.pth'\n",
    "torch.save(best_model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打不開阿!!!\n",
    "# !tensorboard --logdir runs/resnet18_trainer_20241222_125827"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a saved version of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = r\"models\\model_renet18_20241222_141355_4.pth\"\n",
    "saved_model = models.resnet18()\n",
    "saved_model.load_state_dict(torch.load(PATH))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YOLOv8_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
