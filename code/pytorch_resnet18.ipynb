{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn, optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadro RTX 3000 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    cudnn.benchmark = True\n",
    "    device = \"cuda\"\n",
    "    print(torch.cuda.get_device_name())\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"Use CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "## Training set has 100 instances.\n",
      "## Validation set has 50 instances.\n"
     ]
    }
   ],
   "source": [
    "# train 只有 ToTensor()、Normalize()，會發生 overfitt，Valid 無法收斂\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "# Create dataset(use 100 data for my laptop)\n",
    "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform_train, download=True)\n",
    "valid_set = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform_valid, download=True)\n",
    "train_set = Subset(train_set, list(range(0, 100)))\n",
    "valid_set = Subset(valid_set, list(range(100, 150)))\n",
    "\n",
    "# Create data loaders for our datasets\n",
    "train_loader = DataLoader(train_set, batch_size=5, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=5, shuffle=True)\n",
    "\n",
    "print(f'## Training set has {len(train_set)} instances.')\n",
    "print(f'## Validation set has {len(valid_set)} instances.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(weights=\"IMAGENET1K_V1\", progress=True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function(Criterion) & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    sum_loss, sum_acc = 0.0, 0.0\n",
    "    running_loss,running_acc = 0.0, 0.0\n",
    "    last_loss, last_acc = 0.0, 0.0\n",
    "\n",
    "    START_TIME = time()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss\n",
    "        sum_loss += loss\n",
    "        running_acc += accuracy_score(labels.cpu(), outputs.argmax(dim=1).cpu())\n",
    "        sum_acc += accuracy_score(labels.cpu(), outputs.argmax(dim=1).cpu())\n",
    "        if i % 10 == 9:\n",
    "            last_loss = running_loss/10\n",
    "            last_acc = running_acc/10\n",
    "            # print(f' - Batch {i+1} loss: {last_loss:.4f} / accuracy: {last_acc:.4f}')\n",
    "\n",
    "            tb_x = epoch_index * len(train_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            tb_writer.add_scalar('Accuracy/train', last_acc, tb_x)\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "    END_TIME = time()\n",
    "\n",
    "    return sum_loss/(i + 1), sum_acc/(i + 1), (END_TIME-START_TIME)\n",
    "\n",
    "def reset_folder():\n",
    "    shutil.rmtree(\"./models\")\n",
    "    shutil.rmtree(\"./runs\")\n",
    "    os.makedirs(\"./models\")\n",
    "    os.makedirs(\"./runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## start warm up\n",
      "## finished warm up\n",
      "EPOCH 1: Train Loss: 11.7554 / Valid Loss: 26.1470 / Train Accuracy: 0.0100 / Valid Accuracy: 0.0000 --- (0.4155 sec)\n",
      "EPOCH 2: Train Loss: 9.4258 / Valid Loss: 21.9984 / Train Accuracy: 0.0100 / Valid Accuracy: 0.0000 --- (0.4202 sec)\n",
      "EPOCH 3: Train Loss: 7.7873 / Valid Loss: 17.4856 / Train Accuracy: 0.0500 / Valid Accuracy: 0.0000 --- (0.4336 sec)\n",
      "EPOCH 4: Train Loss: 6.6115 / Valid Loss: 15.6699 / Train Accuracy: 0.0700 / Valid Accuracy: 0.0400 --- (0.4184 sec)\n",
      "EPOCH 5: Train Loss: 5.1989 / Valid Loss: 12.3197 / Train Accuracy: 0.1900 / Valid Accuracy: 0.0200 --- (0.4177 sec)\n",
      "EPOCH 6: Train Loss: 4.9147 / Valid Loss: 11.8486 / Train Accuracy: 0.2700 / Valid Accuracy: 0.0200 --- (0.4116 sec)\n",
      "EPOCH 7: Train Loss: 3.4291 / Valid Loss: 12.7229 / Train Accuracy: 0.4000 / Valid Accuracy: 0.0600 --- (0.4219 sec)\n",
      "EPOCH 8: Train Loss: 3.2913 / Valid Loss: 11.7203 / Train Accuracy: 0.3700 / Valid Accuracy: 0.0400 --- (0.4147 sec)\n",
      "EPOCH 9: Train Loss: 3.0244 / Valid Loss: 11.3707 / Train Accuracy: 0.4700 / Valid Accuracy: 0.0600 --- (0.4052 sec)\n",
      "EPOCH 10: Train Loss: 3.0323 / Valid Loss: 11.7328 / Train Accuracy: 0.4700 / Valid Accuracy: 0.0400 --- (0.4233 sec)\n",
      "EPOCH 11: Train Loss: 2.7100 / Valid Loss: 11.6302 / Train Accuracy: 0.3900 / Valid Accuracy: 0.0800 --- (0.4194 sec)\n",
      "EPOCH 12: Train Loss: 2.7012 / Valid Loss: 11.8744 / Train Accuracy: 0.4400 / Valid Accuracy: 0.1000 --- (0.4110 sec)\n",
      "EPOCH 13: Train Loss: 2.9434 / Valid Loss: 10.4524 / Train Accuracy: 0.4200 / Valid Accuracy: 0.1000 --- (0.4179 sec)\n",
      "EPOCH 14: Train Loss: 2.5545 / Valid Loss: 9.4506 / Train Accuracy: 0.4600 / Valid Accuracy: 0.1200 --- (0.4179 sec)\n",
      "EPOCH 15: Train Loss: 2.1058 / Valid Loss: 8.0288 / Train Accuracy: 0.5500 / Valid Accuracy: 0.0800 --- (0.4181 sec)\n",
      "EPOCH 16: Train Loss: 2.1094 / Valid Loss: 7.6646 / Train Accuracy: 0.5200 / Valid Accuracy: 0.0800 --- (0.4154 sec)\n",
      "EPOCH 17: Train Loss: 2.1931 / Valid Loss: 7.0461 / Train Accuracy: 0.4800 / Valid Accuracy: 0.1200 --- (0.4177 sec)\n",
      "EPOCH 18: Train Loss: 1.5711 / Valid Loss: 7.2300 / Train Accuracy: 0.6100 / Valid Accuracy: 0.1200 --- (0.4163 sec)\n",
      "EPOCH 19: Train Loss: 1.6060 / Valid Loss: 7.8145 / Train Accuracy: 0.5600 / Valid Accuracy: 0.1000 --- (0.4258 sec)\n",
      "EPOCH 20: Train Loss: 1.8467 / Valid Loss: 7.4299 / Train Accuracy: 0.5900 / Valid Accuracy: 0.1800 --- (0.4730 sec)\n",
      "== Total time: 8.4146 sec ==\n"
     ]
    }
   ],
   "source": [
    "reset_folder()\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter(f'runs/resnet18_trainer_{timestamp}')\n",
    "epoch_number = 0\n",
    "\n",
    "total_time = 0\n",
    "EPOCHS = 20\n",
    "best_vloss = 1_000_000.0\n",
    "\n",
    "# warm up \n",
    "print(f'## start warm up')\n",
    "dummy_data = torch.randn(5, 3, 32, 32).to(device)\n",
    "for _ in range(500):\n",
    "    _ = model(dummy_data)\n",
    "print(f'## finished warm up')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'EPOCH {epoch_number+1}: ', end=\"\")\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss, avg_acc, train_time = train_one_epoch(epoch_number, writer)\n",
    "    total_time += train_time\n",
    "\n",
    "    # Set the model to evaluation model\n",
    "    model.eval()\n",
    "    sum_vloss = 0.0\n",
    "    sum_vacc = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(valid_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            vinputs, vlabels = vinputs.to(device), vlabels.to(device)\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels) # current batch valid loss\n",
    "            vacc = accuracy_score(vlabels.cpu(), voutputs.argmax(dim=1).cpu()) # current batch valid accuracy\n",
    "            sum_vloss += vloss.item() # running_vloss\n",
    "            sum_vacc += vacc # running_vacc\n",
    "\n",
    "    avg_vloss = sum_vloss / (i + 1)\n",
    "    avg_vacc = sum_vacc / (i + 1)\n",
    "    print(f'Train Loss: {avg_loss:.4f} / Valid Loss: {avg_vloss:.4f} / '\n",
    "          f'Train Accuracy: {avg_acc:.4f} / Valid Accuracy: {avg_vacc:.4f} --- ({train_time:.4f} sec)')\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.add_scalars('Training vs. Validation Accuracy', \n",
    "                    { 'Training' : avg_acc, 'Validation': avg_vacc},\n",
    "                    epoch_number + 1)\n",
    "    writer.flush() # immediately write into file\n",
    " \n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_model = model\n",
    "        best_vloss = avg_vloss\n",
    "\n",
    "    epoch_number += 1\n",
    "\n",
    "print(f'== Total time: {total_time:.4f} sec ==')\n",
    "model_path = f'models/model_renet18_{timestamp}_{epoch_number}.pth'\n",
    "torch.save(best_model.state_dict(), model_path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a saved version of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = r\"models\\model_renet18_.pth\"\n",
    "# saved_model = models.resnet18()\n",
    "# saved_model.load_state_dict(torch.load(PATH))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YOLOv8_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
